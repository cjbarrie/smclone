if (status_code == 503) {
count <- count + 1
Sys.sleep(count * 5)
}
if (status_code == 429) {
.trigger_sleep(r, verbose = verbose)
count <- count + 1
}
}
jsonlite::fromJSON(httr::content(r, "text"))
}
dat <- make_query(url = requrl, params = params, bearer_token = bearer_token, verbose = verbose)
make_query <- function(url, params, bearer_token, max_error = 4, verbose = TRUE) {
bearer_token <- check_bearer(bearer_token)
count <- 0
while (TRUE) {
if (count >= max_error) {
stop("Too many errors.")
}
r <- httr::GET(url, httr::add_headers(Authorization = bearer_token), query = params)
status_code <- httr::status_code(r)
if (!status_code %in% c(200, 429, 503)) {
stop(paste("something went wrong. Status code:", httr::status_code(r)))
}
if (httr::headers(r)$`x-rate-limit-remaining` == "1") {
.vwarn(verbose, paste("x-rate-limit-remaining=1. Resets at", .check_reset(r)))
count <- count + 1
}
if (status_code == 200) {
break()
}
if (status_code == 503) {
count <- count + 1
Sys.sleep(count * 5)
}
if (status_code == 429) {
.trigger_sleep(r, verbose = verbose)
count <- count + 1
}
}
jsonlite::fromJSON(httr::content(r, "text"))
}
get_tweets <- function(params, endpoint_url, page_token_name = "next_token", n, file, bearer_token, data_path, export_query, bind_tweets, verbose) {
# Check file storage conditions
create_storage_dir(data_path = data_path, export_query = export_query, built_query = params[["query"]], start_tweets = params[["start_time"]], end_tweets = params[["end_time"]], verbose = verbose)
# Start Data Collection Loop
next_token <- ""
df.all <- data.frame()
toknum <- 0
ntweets <- 0
while (!is.null(next_token)) {
df <- make_query(url = endpoint_url, params = params, bearer_token = bearer_token, verbose = verbose)
if (is.null(data_path)) {
# if data path is null, generate data.frame object within loop
df.all <- dplyr::bind_rows(df.all, df$data)
}
if (!is.null(data_path) & is.null(file) & !bind_tweets) {
df_to_json(df, data_path)
}
if (!is.null(data_path)) {
df_to_json(df, data_path)
df.all <- dplyr::bind_rows(df.all, df$data) #and combine new data with old within function
}
next_token <- df$meta$next_token #this is NULL if there are no pages left
if (!is.null(next_token)) {
params[[page_token_name]] <- next_token
}
toknum <- toknum + 1
if (is.null(df$data)) {
n_newtweets <- 0
} else {
n_newtweets <- nrow(df$data)
}
ntweets <- ntweets + n_newtweets
.vcat(verbose, "Total pages queried: ", toknum, " (tweets captured this page: ", n_newtweets, ").\n",
sep = ""
)
if (ntweets >= n){ # Check n
df.all <- df.all[1:n,] # remove extra
.vcat(verbose, "Total tweets captured now reach", n, ": finishing collection.\n")
break
}
if (is.null(next_token)) {
.vcat(verbose, "This is the last page for", params[["query"]], ": finishing collection.\n")
break
}
}
if (is.null(data_path) & is.null(file)) {
return(df.all) # return to data.frame
}
if (!is.null(file)) {
saveRDS(df.all, file = file) # save as RDS
return(df.all) # return data.frame
}
if (!is.null(data_path) & bind_tweets) {
return(df.all) # return data.frame
}
if (!is.null(data_path) &
is.null(file) & !bind_tweets) {
.vcat(verbose, "Data stored as JSONs: use bind_tweets function to bundle into data.frame")
}
}
## get_tweets <- function(q="",page_n=500,start_time,end_time,token,next_token="", verbose = TRUE){
##   # if(n>500){
##   #   warning("n too big. Using 500 instead")
##   #   n <- 500
##   # }
##   # if(n<5){
##   #   warning("n too small Using 10 instead")
##   #   n <- 500
##   # }
##   if(missing(start_time)){
##     stop("start time must be specified.")
##   }
##   if(missing(end_time)){
##     stop("end time must be specified.")
##   }
##   if(missing(token)){
##     stop("bearer token must be specified.")
##   }
##   if(substr(token,1,7)=="Bearer "){
##     bearer <- token
##   } else{
##     bearer <- paste0("Bearer ",token)
##   }
##   #endpoint
##   url <- "https://api.twitter.com/2/tweets/search/all"
##   #parameters
##   params <- list(
##     "query" = q,
##     "max_results" = page_n,
##     "start_time" = start_time,
##     "end_time" = end_time,
##     "tweet.fields" = "attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,source,text,withheld",
##     "user.fields" = "created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld",
##     "expansions" = "author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id",
##     "place.fields" = "contained_within,country,country_code,full_name,geo,id,name,place_type"
##   )
##   if(next_token!=""){
##     params[["next_token"]] <- next_token
##   }
##   r <- httr::GET(url,httr::add_headers(Authorization = bearer),query=params)
##   #fix random 503 errors
##   count <- 0
##   while(httr::status_code(r)==503 & count<4){
##     r <- httr::GET(url,httr::add_headers(Authorization = bearer),query=params)
##     count <- count+1
##     Sys.sleep(count*5)
##   }
##   if(httr::status_code(r)==429){
##     .vcat(verbose, "Rate limit reached, sleeping... \n")
##     Sys.sleep(900)
##     r <- httr::GET(url,httr::add_headers(Authorization = bearer),query=params)
##   }
##   if(httr::status_code(r)!=200){
##     stop(paste("something went wrong. Status code:", httr::status_code(r)))
##   }
##   if(httr::headers(r)$`x-rate-limit-remaining`=="1"){
##     .vwarn(verbose, paste("x-rate-limit-remaining=1. Resets at",as.POSIXct(as.numeric(httr::headers(r)$`x-rate-limit-reset`), origin="1970-01-01")))
##   }
##   dat <- jsonlite::fromJSON(httr::content(r, "text"))
##   dat
## }
# fetch_data <- function(built_query, data_path, file, bind_tweets, start_tweets, end_tweets, bearer_token = get_bearer(), n, page_n, verbose){
#   nextoken <- ""
#   df.all <- data.frame()
#   toknum <- 0
#   ntweets <- 0
#   while (!is.null(nextoken)) {
#     df <-
#       get_tweets(
#         q = built_query,
#         page_n = page_n,
#         start_time = start_tweets,
#         end_time = end_tweets,
#         bearer_token = bearer_token,
#         next_token = nextoken
#       )
#     if (is.null(data_path)) {
#       # if data path is null, generate data.frame object within loop
#       df.all <- dplyr::bind_rows(df.all, df$data)
#     }
#
#     if (!is.null(data_path) & is.null(file) & bind_tweets == F) {
#       df_to_json(df, data_path)
#     }
#     if (!is.null(data_path)) {
#       df_to_json(df, data_path)
#       df.all <-
#         dplyr::bind_rows(df.all, df$data) #and combine new data with old within function
#     }
#
#     nextoken <-
#       df$meta$next_token #this is NULL if there are no pages left
#     toknum <- toknum + 1
#     if (is.null(df$data)) {
#       n_newtweets <- 0
#     } else {
#       n_newtweets <- nrow(df$data)
#     }
#     ntweets <- ntweets + n_newtweets
#     .vcat(verbose,
#         "query: <",
#         built_query,
#         ">: ",
#         "(tweets captured this page: ",
#         n_newtweets,
#         "). Total pages queried: ",
#         toknum,
#         ". Total tweets ingested: ",
#         ntweets,
#         ". \n",
#         sep = ""
#         )
#     if (ntweets > n){ # Check n
#       df.all <- df.all[1:n,] # remove extra
#       .vcat(verbose, "Total tweets ingested now exceeds ", n, ": finishing collection.\n")
#       break
#     }
#     if (is.null(nextoken)) {
#       .vcat(verbose, "This is the last page for", built_query, ": finishing collection.\n")
#       break
#     }
#   }
#
#   if (is.null(data_path) & is.null(file)) {
#     return(df.all) # return to data.frame
#   }
#   if (!is.null(file)) {
#     saveRDS(df.all, file = file) # save as RDS
#     return(df.all) # return data.frame
#   }
#
#   if (!is.null(data_path) & bind_tweets) {
#     return(df.all) # return data.frame
#   }
#
#   if (!is.null(data_path) &
#       is.null(file) & !bind_tweets) {
#     .vcat(verbose, "Data stored as JSONs: use bind_tweets function to bundle into data.frame")
#   }
# }
check_bearer <- function(bearer_token){
if(missing(bearer_token)){
stop("bearer token must be specified.")
}
if(substr(bearer_token,1,7)=="Bearer "){
bearer <- bearer_token
} else{
bearer <- paste0("Bearer ",bearer_token)
}
return(bearer)
}
check_data_path <- function(data_path, file, bind_tweets, verbose = TRUE){
#warning re data storage recommendations if no data path set
if (is.null(data_path)) {
.vwarn(verbose, "Recommended to specify a data path in order to mitigate data loss when ingesting large amounts of data.")
}
#warning re data.frame object and necessity of assignment
if (is.null(data_path) & is.null(file)) {
.vwarn(verbose, "Tweets will not be stored as JSONs or as a .rds file and will only be available in local memory if assigned to an object.")
}
#stop clause for if user sets bind_tweets to FALSE but sets no data path
if (is.null(data_path) & !bind_tweets) {
stop("Argument (bind_tweets = FALSE) only valid when a data_path is specified.")
}
#warning re binding of tweets when a data path and file path have been set but bind_tweets is set to FALSE
if (!is.null(data_path) & !is.null(file) & !bind_tweets) {
.vwarn(verbose, "Tweets will still be bound in local memory to generate .rds file. Argument (bind_tweets = FALSE) only valid when just a data path has been specified.")
}
#warning re data storage and memory limits when setting bind_tweets to TRUE
if (!is.null(data_path) & is.null(file) & bind_tweets) {
.vwarn(verbose, "Tweets will be bound in local memory as well as stored as JSONs.")
}
}
create_data_dir <- function(data_path, verbose = TRUE){
#create folders for storage
if (dir.exists(file.path(data_path))) {
.vwarn(verbose, "Directory already exists. Existing JSON files may be parsed and returned, choose a new path if this is not intended.")
invisible(data_path)
}
dir.create(file.path(data_path), showWarnings = FALSE)
invisible(data_path)
}
df_to_json <- function(df, data_path){
# check input
# if data path is supplied and file name given, generate data.frame object within loop and JSONs
jsonlite::write_json(df$data,
file.path(data_path, paste0("data_", df$data$id[nrow(df$data)], ".json")))
jsonlite::write_json(df$includes,
file.path(data_path, paste0("users_", df$data$id[nrow(df$data)], ".json")))
}
create_storage_dir <- function(data_path, export_query, built_query, start_tweets, end_tweets, verbose){
if (!is.null(data_path)){
create_data_dir(data_path, verbose)
if (isTRUE(export_query)){ # Note export_query is called only if data path is supplied
# Writing query to file (for resuming)
filecon <- file(file.path(data_path, "query"))
writeLines(c(built_query,start_tweets,end_tweets), filecon)
close(filecon)
}
}
}
.gen_random_dir <- function() {
file.path(tempdir(), paste0(sample(letters, 20), collapse = ""))
}
.vcat <- function(bool, ...) {
if (bool) {
cat(...)
}
}
.vwarn <- function(bool, ...) {
if (bool) {
warning(..., call. = FALSE)
}
}
.check_reset <- function(r, tzone = "") {
lubridate::with_tz(lubridate::as_datetime(as.numeric(httr::headers(r)$`x-rate-limit-reset`), tz = tzone), tzone)
}
.trigger_sleep <- function(r, verbose = TRUE, really_sleep = TRUE, ref_time = Sys.time(), tzone = "") {
reset_time <- .check_reset(r, tzone = tzone)
## add 1s as buffer
sleep_period <- ceiling(as.numeric(reset_time - ref_time, units = "secs")) + 1
.vcat(verbose, "Rate limit reached. Rate limit will reset at", as.character(reset_time) ,"\nSleeping for", sleep_period ,"seconds. \n")
if (verbose) {
pb <- utils::txtProgressBar(min = 0, max = sleep_period, initial = 0)
for (i in seq_len(sleep_period)) {
utils::setTxtProgressBar(pb, i)
if (really_sleep) {
Sys.sleep(1)
}
}
} else {
if (really_sleep) {
Sys.sleep(sleep_period)
}
}
invisible(r)
}
.process_qparam <- function(param, param_str,query) {
if(!is.null(param)){
if(isTRUE(param)) {
query <- paste(query, param_str)
} else if(param == FALSE) {
query <- paste(query, paste0("-", param_str))
}
}
return(query)
}
add_query_prefix <- function(x, prefix){
q <- paste0(prefix, x)
q <- paste(q, collapse = " OR ")
q <- paste0("(",q,")")
return(q)
}
dat <- make_query(url = requrl, params = params, bearer_token = bearer_token, verbose = verbose)
usethis::edit_r_environ()
bearer_token <- "AAAAAAAAAAAAAAAAAAAAAPw%2BJQEAAAAAq5Ot8ttyYlAqT9nLMuVuR1jI5fA%3DqG9HkIUXWFFlLLDVC6G0PFo4shkDVg02DwVxGQIVKvhPVE3vdV"
dat <- make_query(url = requrl, params = params, bearer_token = bearer_token, verbose = verbose)
requrl <- paste0(url,x[i],endpoint)
x <- 333357345
dat <- make_query(url = requrl, params = params, bearer_token = bearer_token, verbose = verbose)
requrl <- paste0(url,x[i],endpoint)
i <- 1
requrl <- paste0(url,x[i],endpoint)
dat <- make_query(url = requrl, params = params, bearer_token = bearer_token, verbose = verbose)
next_token <- dat$meta$next_token #this is NULL if there are no pages left
new_rows <- dat$data
new_rows$from_id <- x[i]
View(new_rows)
new_rows$id[nrow(new_rows$id)]
new_rows$id[nrow(new_rows$id),]
new_rows$id[nrow(new_rows)]
source("utils.R")
library(dplyr)
set.seed(123L)
options(scipen = 999)
source("utils.R")
devtools::install_github("soodoku/tuber", build_vignettes = TRUE)
install.packages("devtools")
devtools::install_github("soodoku/tuber", build_vignettes = TRUE)
library(tuber)
yt_oauth("169868713938-vo1p37ive6sin3je74of5ovnbictuhlh.apps.googleusercontent.com",
"8wMmoTuX-DbwBYAAtPoAZ80a")
library(dplyr)
library(tuber)
#get recommended videos
startvid <- "IlobwWaDAfM"
rel_vids <- get_related_videos(startvid, max_results = 50)
tweets <- read.csv("/Users/cbarrie6/Dropbox/pd_projects/MP_enviro/data/output/mptweetsdat17.csv")
colnames(tweets)
library(dplyr)
tweets_short <- tweets %>%
select(id, user_id, username, name, tweet, retweets_count, likes_count)
tweets_short_sample <- tweets_short %>%
sample_n(20)
write.csv(tweets_short_sample, "/Users/cbarrie6/Dropbox/sandbox/smclone/backend/tweets.csv")
colnames(tweets)
colnames(tweets_short_sample)
str(tweets_short_sample)
View(tweets_short_sample)
options(scipen = 999)
write.csv(tweets_short_sample, "/Users/cbarrie6/Dropbox/sandbox/smclone/backend/tweets.csv")
str(tweets)
str(tweets_short_sample)
colnames(tweets)
head(tweets$link)
load("/Users/cbarrie6/Dropbox/pd_projects/MP_enviro/data/output/MPtweetsv2to_add.RData")
>
colnames(df.allto_add)
library(academictwitteR)
users <- unique(tweets$username)
tweets <- tweets <- get_all_tweets(
users = users[[i]],
start_tweets = "2023-04-23T00:00:00Z",
end_tweets = "2023-04-26T00:00:00Z",
is_retweet = F,
bind_tweets = T,
n = 100 # max. 1k tweets per user
)
tweets <- tweets <- get_all_tweets(
users = users[[1]],
start_tweets = "2023-04-23T00:00:00Z",
end_tweets = "2023-04-26T00:00:00Z",
is_retweet = F,
bind_tweets = T,
n = 100 # max. 1k tweets per user
)
View(tweets)
View(tweets[[7]][[1]])
tweets[[7]][[1]][[1]][["display_url"]]
View(tweets)
str(tweets)
users <- unique(tweets$username)
tweets_all <- data.frame()
for (i in seq_along(users)) {
get_all_tweets(
users = users[[1]],
start_tweets = "2023-04-23T00:00:00Z",
end_tweets = "2023-04-26T00:00:00Z",
is_retweet = F,
bind_tweets = F,
data_path = "tweetdata/",
n = 100 # max. 1k tweets per user
)
}
getwd()
setwd("/Users/cbarrie6/Dropbox/sandbox/smclone/getdata.R")
setwd("/Users/cbarrie6/Dropbox/sandbox/smclone/")
users <- unique(tweets$username)
tweets <- read.csv("/Users/cbarrie6/Dropbox/pd_projects/MP_enviro/data/output/mptweetsdat17.csv")
users <- unique(tweets$username)
for (i in seq_along(users)) {
get_all_tweets(
users = users[[i]],
start_tweets = "2023-01-01T00:00:00Z",
end_tweets = "2023-04-30T00:00:00Z",
is_retweet = F,
bind_tweets = F,
data_path = "tweetdata/",
n = 100 # max. 100 tweets per user
)
}
users <- bind_tweets("tweetdate/", user = T)
users <- bind_tweets("tweetdata/", user = T)
View(users)
users <- unique(tweets$username)
for (i in seq_along(users)) {
get_all_tweets(
users = users[[i]],
start_tweets = "2023-01-01T00:00:00Z",
end_tweets = "2023-04-30T00:00:00Z",
is_retweet = F,
bind_tweets = F,
data_path = "tweetdata/",
n = 100 # max. 100 tweets per user
)
}
users <- bind_tweets("tweetdata/", user = T)
tweets <- bind_tweets("tweetdata/")
colnames(tweets)
tweets$retweets_count <- tweets$public_metrics$retweet_count
tweets$likes_count <- tweets$public_metrics$like_count
tweets$user_id <- tweets$author_id
tweets$tweet <- tweets$text
tweets_short <- tweets %>%
select(id, user_id, tweet, retweets_count, likes_count)
colnames(users)
users_short <- users %>%
select(username, name, profile_image_url)
users_short <- users %>%
select(id, username, name, profile_image_url)
users_short$user_id <- users_short$id
users_short <- users %>%
select(user_id, username, name, profile_image_url)
users$user_id <- users$id
users_short <- users %>%
select(user_id, username, name, profile_image_url)
View(users_short)
library(tidylog)
tweets_short %>%
left_join(users_short, by = "user_id")
library(tidylog)
tweets_joined <- tweets_short %>%
left_join(users_short, by = "user_id")
View(tweets_joined)
tweets_joined_sample <- tweets_joined %>%
sample_n(100)
write.csv(tweets_short_sample, "/Users/cbarrie6/Dropbox/sandbox/smclone/backend/tweets.csv")
write.csv(tweets_joined_sample, "/Users/cbarrie6/Dropbox/sandbox/smclone/backend/tweets.csv")
View(tweets_joined_sample)
head(tweets_joined_sample$profile_image_url)
